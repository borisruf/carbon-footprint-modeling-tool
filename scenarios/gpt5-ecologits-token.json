{
   "title":"gpt-5 emission per output token (EcoLogits)",
   "description":"The nonprofit organization <a href='https://genai-impact.org' target='_blank'>GenAI Impact</a> takes GPU and server power consumption into account when modeling AI model emissions. In both cases, they use the open dataset from the <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>LLM Perf Leaderboard</a> produced by Hugging Face. For closed models such as gpt-5, where the data is not available, they fit linear regression models that model the energy consumption per output token as a function of the number of active parameters. To assess server energy consumption excluding GPUs, they fit a linear regression model to estimate the token generation latency.<br/><br/>To account for data center infrastructure such as the cooling equipment, a Power Usage Effectiveness (PUE) factor of <b>1.2x</b> is applied.",
   "scopes":[
      {
         "level":"Scope 2",
         "description":{
            
         },
         "list":[
            {
               "type":"link",
               "quantity":1.2,
               "scenario_id":"gpt5-ecologits-token-2"
            }
         ]
      },
      {
         "level":"Scope 3",
         "description":{
            
         },
         "list":[
            {
               "type":"component",
               "consumer":{
                  "name":"Hardware",
                  "description":"Server and GPUs",
                  "consumptions":[
                     
                  ]
               },
               "quantity":"1",
               "quantity_unit":"token",
               "source":{
                  "name":"Embodied carbon",
                  "description":"Global warming potential (GWP)",
                  "emissions":{
                     "co2e":{
                        "value":"1.405381e-06",
                        "unit":"kg",
                        "base_unit":"token",
                        "reference_url":"https://ecologits.ai/0.8/methodology/llm_inference/"
                     }
                  }
               }
            }
         ]
      }
   ]
}