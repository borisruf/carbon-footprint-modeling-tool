{
  "title": "Carbon footprint of one average query to GPT-3",
  "description": "In the absence of reliable data on the energy consumption of GPT-3, Ludvigsen relates to the large language model <a href=\"https://huggingface.co/bigscience/bloom\" target=\"_blank\">BLOOM</a> instead. Both models are roughly the same size â€” 175b vs 176b parameters for GPT-3 and BLOOM respectively. <br/><br/>The experiment handled 230,768 queries in total, so the fraction for one request is 1/230768 = <a href='https://www.google.com/search?q=1%2F230768' target='_blank'>4.3334e-6</a>",
  "scopes": [
    {
      "level": "Scope 3",
      "description": {},
        "list": [
        {
          "type": "link",
          "quantity": 0.000004333356445,
          "scenario_id": "gpt-kasper-2"
        }
      ]
    }
  ]
}
