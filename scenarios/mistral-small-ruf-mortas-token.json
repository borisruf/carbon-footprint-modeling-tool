{
    "title": "Mistral Small emission per token (Ruf & Mortas)",
    "description": "The Mistral Small model is not listed on the Hugging Face <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>LLM-Perf Leaderboard</a> which benchmarks the performance of large language models (LLMs) and measures the energy consumption using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. However, as Mistral <a href='https://docs.mistral.ai/getting-started/models/weights/' target='_blank'>open source</a> the weights of their models, we know that Mistral Small has <a href='https://docs.mistral.ai/getting-started/models/weights/#sizes' target='_blank'>22B parameters</a>.<br/><br/>The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>leaderboard</a> does provide energy information for <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral 7B v0.1</a> which has <a href='https://docs.mistral.ai/getting-started/models/weights/#sizes' target='_blank'>7.3B parameters</a>. Accordingly, we propose to extrapolate this data and approximate the energy consumption of Mistral Small by applying a factor of <b><a href='https://www.google.com/search?q=22/7.3' target='_blank'>3</a>x</b>.",
    "scopes": [
      {
      "level": "Scope 3",
      "description": {},
        "list": [
        {
          "type": "link",
          "quantity": 3,
          "scenario_id": "mistral-7B-token"
        }
      ]
    }
    ]
  }
  
