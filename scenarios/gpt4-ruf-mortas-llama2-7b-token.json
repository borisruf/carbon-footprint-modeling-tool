{
  "title": "GPT-4 emission per token (Ruf & Mortas)",
  "description": "There is limited public information available about the technical structure of GPT4. <a href=\"https://x.com/swyx/status/1671272883379908608\" target=\"blank\">Some sources</a> suggest it is a <a href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" target=\"_blank\">Mixture of Expert</a> (MoE) with 8 x 220 billion parameters.<br/><br/>In other MoE architectures with disclosed weights, it has been noted that the actual total number of parameters is lower than indicated by the architecture. Additionally, it has been observed that only a fraction of those parameters are actually active during inference. For example, the <a href=\"https://docs.mistral.ai/getting-started/models/weights/#sizes/\" target=\"_blank\">Mixtral-8x22B-v0.3</a> architecture reports a total of 140.6B parameters (<a href=\"https://www.google.com/search?q=140.5/(8*22)\">80%</a>), out of which 39.1B parameters are activated during inference (<a href=\"https://www.google.com/search?q=39.1/140.6\" target=\"_blank\">28%</a>).<br/><br/>According to this information, we assume the same proportions for GPT-4 and thus estimate a total of <a href=\"https://www.google.com/search?q=8*220*0.8*0.28\" target=\"_blank\">394.24</a> billion active parameters.<br/><br/>Finally, we extrapolate the energy consumption of GPT-4 based on a scenario for GPT-3.5 which is known to have <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a> parameters, and therefore apply a factor of <a href=\"https://www.google.clom/search?q=394.24/175\" target=\"_blank\"><b>2.25x</b></a>.",

  "scopes": [
    {
      "level": "Scope 3",
      "description": {},
        "list": [
        {
          "type": "link",
          "quantity": 2.25,
          "scenario_id": "gpt3-ruf-mortas-llama2-7b-token"
        }
      ]
    }
  ]
}