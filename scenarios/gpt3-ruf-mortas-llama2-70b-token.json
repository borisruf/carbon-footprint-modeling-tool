{
  "title": "GPT-3.5 per token â€“ Llama2 70B (Ruf & Mortas)",
  "description": "In the absence of reliable data on the energy consumption of GPT-3.5, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" target=\"_blank\">Llama 2 70B</a>. The number of parameters in Llama 2 70B is 68.98 billion, whereas GPT-3.5 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b><a href=\"https://www.google.com/search?q=175/70\" target=\"blank\">2.5x</a></b> more energy.",
  "scopes": [
    {
      "level": "Scope 3",
      "description": {},
        "list": [
        {
          "type": "link",
          "quantity": 2.5,
          "scenario_id": "llama2-70b-token"
        }
      ]
    }
  ]
}
