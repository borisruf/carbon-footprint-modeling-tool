{
  "title": "Carbon footprint of one average query to GPT-3",
  "description": "In the absence of reliable data on the energy consumption of GPT-3, Ludvigsen used a freely available open source language model (<a href=\"https://huggingface.co/bigscience/bloom\" target=\"_blank\">BLOOM</a>). The models are roughly the same size â€” 175b vs 176b parameters for GPT-3 and BLOOM respectively. So, he supposed that their energy consumption should be quite similar.<br/><br/>Ludvigsen used the <a href=\"https://arxiv.org/pdf/2211.02001.pdf\" target=\"_blank\">experiment</a> from Alexandra Sasha Luccioni, Sylvain Viguier and Anne-Laure Ligozat : they calculate the carbon foorprint of the BLOOM model API. This experiment was running during 18 days, and had 230768 queries.<br/><br/>The fraction of such an operation of one query is 1/230768 = <a href='https://www.google.com/search?q=1%2F230768' target='_blank'>4.3334e-6</a>",
  "scopes": [
    {
      "level": "Scope 3",
      "description": {},
        "list": [
        {
          "type": "link",
          "quantity": 0.000004333356445,
          "scenario_id": "scenario-gpt-kasper-n-2"
        }
      ]
    }
  ]
}
