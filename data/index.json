[
  {
    "id": "diesel",
    "title": "1 litre of diesel",
    "description": "",
    "emissions": {
      "co2e": 2.71685,
      "co2": 2.6533
    }
  },
  {
    "id": "falcon-180b-ruf-mortas-token",
    "title": "Falcon 180B emission per token (Ruf & Mortas)",
    "description": "In the absence of energy consumption data of Falcon 180B, this information is extrapolated from available data on <a href=\"https://huggingface.co/tiiuae/falcon-7b\" target=\"_blank\">Falcon 7B</a>. The number of parameters in Falcon 7B is 7 billion, whereas Falcon 180B has a much larger parameter count of 180 billion</a>.<br/><br/>Therefore, we assume that submitting a query to Falcon 180B requires <b><a href=\"https://www.google.com/search?q=180/7\" target=\"blank\">25x</a></b> more energy.",
    "emissions": {
      "co2e": 0.000009522124747329
    }
  },
  {
    "id": "falcon-7b-token",
    "title": "Falcon 7B emission per token (Hugging Face)",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Falcon model with 7B parameters handles 422,919 completion tokens/kWh.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.<br/><br/>Per token, that's (1/422919)x0.5 = <a href='https://www.google.com/search?q=%281%2F422919%29x0.5' target='_blank'>1.2e-6</a> kWh.",
    "emissions": {
      "co2e": 3.8088498989316e-7
    }
  },
  {
    "id": "gpt-ludvigsen-0",
    "title": "Monthly carbon footprint of ChatGPT (Ludvigsen)",
    "description": "Kasper Groes Albin Ludvigsen <a href='https://towardsdatascience.com/chatgpts-electricity-consumption-7873483feac4' target='_blank'>approximated</a> the carbon footprint of ChatGPT. The calculation and the underlying assumptions he made are further illustrated in this <a href='https://github.com/borisruf/carbon-footprint-modeling-tool' target='_blank'>carbon footprint scenario</a>.<br/><br/>In January 2023, ChatGPT had about 100M unique users who made <a href='https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app' target='_blank'>590M</a> queries to GPT-3. The estimated emissions associated with these requests have been modeled below. To explore further, adjust the quantity in the input field or click on the link.",
    "emissions": {
      "co2e": 752841.713050906
    }
  },
  {
    "id": "gpt-ludvigsen-1",
    "title": "Carbon footprint of one average query to GPT-3",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, Ludvigsen relates to the large language model <a href=\"https://huggingface.co/bigscience/bloom\" target=\"_blank\">BLOOM</a> instead. Both models are roughly the same size â€” 175b vs 176b parameters for GPT-3 and BLOOM respectively. <br/><br/>The experiment handled 230,768 queries in total, so the fraction for one request is 1/230768 = <a href='https://www.google.com/search?q=1%2F230768' target='_blank'>4.3334e-6</a>",
    "emissions": {
      "co2e": 0.0012760029034761119
    }
  },
  {
    "id": "gpt-ludvigsen-2",
    "title": "BLOOM model experiment",
    "description": "Alexandra Sasha Luccioni et al. published a <a href='https://arxiv.org/pdf/2211.02001.pdf' target='_blank'>study</a> on the the carbon footprint of the <a href='https://bigscience.huggingface.co/blog/bloom' target='_blank'>BLOOM model</a>. With its 176 billion parameters, BLOOM is a large language model (LLM) that is able to generate text in 46 natural languages and 13 programming languages. The researchers ran their experiment on 16 Nvidia A100 GPUs during 18 days. The model handled <b>230,768 queries</b> in the course of the experiment. In total, the system consumed <b>914 kWh</b> of electriciy for CPU, RAM and GPU usage.<br/><br/>For the emissions, we used a grid emissions factor for Western USA (WECC) from <a href='https://www.cloudcarbonfootprint.org/docs/methodology/#azure-2' target='_blank'>EPA</a>.  ",
    "emissions": {
      "co2e": 294.46063799999996
    }
  },
  {
    "id": "gpt-pointon-0",
    "title": "Monthly carbon footprint of ChatGPT (Pointon)",
    "description": "Chris Pointon <a href='https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a' target='_blank'>approximated</a> the carbon footprint of ChatGPT. The calculation and the underlying assumptions he made are further illustrated in this <a href='https://github.com/borisruf/carbon-footprint-modeling-tool' target='_blank'>carbon footprint scenario</a>.<br/><br/>In January 2023, ChatGPT had about 100M unique users who made <a href='https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app' target='_blank'>590M</a> queries to GPT-3. The estimated emissions associated with these requests have been modeled below. To explore further, adjust the quantity in the input field or click on the link.",
    "emissions": {
      "co2e": 225639.04983003586
    }
  },
  {
    "id": "gpt-pointon-1",
    "title": "Carbon footprint of one average query to GPT-3 (Pointon)",
    "description": "Chris Pointon assumes that each query to ChatGPT generates in average <a href='https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a' target='_blank'>30 words</a>.",
    "emissions": {
      "co2e": 0.00038243906750853537
    }
  },
  {
    "id": "gpt-pointon-2",
    "title": "Carbon emission of 1 output word generated by GPT-3",
    "description": "Tom Goldstein, Professor at U of Maryland, extrapolates that if GPT-3 were to run on a single Nvidia A100 GPU, it would take <a href=\"https://twitter.com/tomgoldsteincs/status/1600196995389366274\" target=\"_blank\">350 ms</a> to output one word.<br/><br/>The fraction of such an operation of one hour is 350/3600000 = <a href='https://www.google.com/search?q=350%2F3600000' target='_blank'>9.7222e-5</a>",
    "emissions": {
      "co2e": 0.000012747968916951178
    }
  },
  {
    "id": "gpt-pointon-3",
    "title": "Hourly emission by GPU running at full load",
    "description": "The <a href='https://medium.com/@chrispointon/the-carbon-footprint-of-chatgpt-e1bc14e4cc2a' target='_blank'>author assumes</a> that ChatGPT is hosted in an Azure datacenter in California, running on Nvidia Tesla A100 GPUs.<br/><br/>Thus, he takes as starting point the energy consumption of this processor under full load as provided by Cloud Carbon Footprint (<a href='https://www.cloudcarbonfootprint.org/docs/methodology/#appendix-iii-gpus-and-minmax-watts' target='_blank'>407 Watts per hour</a>). For the emissions, he uses a grid emissions factor for Western USA (WECC) from <a href='https://www.cloudcarbonfootprint.org/docs/methodology/#azure-2' target='_blank'>EPA</a>.  ",
    "emissions": {
      "co2e": 0.13112196899999998
    }
  },
  {
    "id": "gpt-pointon-token",
    "title": "GPT-3 emission per token (Pointon)",
    "description": "Tom Goldstein, Professor at U of Maryland, extrapolates that if GPT-3 were to run on a single Nvidia A100 GPU, it would take <a href=\"https://twitter.com/tomgoldsteincs/status/1600196995389366274\" target=\"_blank\">350 ms</a> to output one word.<br/><br/>The fraction of such an operation of one hour is <b>350/3600000</b> = <a href='https://www.google.com/search?q=350%2F3600000' target='_blank'>9.7222e-5</a><br/><br/>As this calculation ignores the impact of the prompt on the energy consumption, we assume an equal number of prompt tokens and apply factor <b>0.5x</b> to obtain a per-token value.",
    "emissions": {
      "co2e": 0.000006373984458475589
    }
  },
  {
    "id": "gpt-ruf-mortas-0",
    "title": "Monthly carbon footprint of ChatGPT (Ruf & Mortas)",
    "description": "In this scenario, we approximate the carbon footprint of ChatGPT based on the Llama 2 model and energy consumption data from Hugging Face.",
    "emissions": {
      "co2e": 355171.51176382165
    }
  },
  {
    "id": "gpt-ruf-mortas-1",
    "title": "Carbon footprint of one average query to GPT-3 (Ruf & Mortas)",
    "description": "We assume that each query to ChatGPT generates in average 1,319 tokens.",
    "emissions": {
      "co2e": 0.0006019856131590197
    }
  },
  {
    "id": "gpt-ruf-mortas-2",
    "title": "GPT-3",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" target=\"_blank\">Llama 2</a>. The number of parameters in Llama 2 is 68.98 billion, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b>2.5x</b> more energy.",
    "emissions": {
      "co2e": 4.5639546107582997e-7
    }
  },
  {
    "id": "gpt-ruf-mortas-3",
    "title": "Llama 2",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Llama 2 model with 70B parameters handles 163,934 completion tokens/kWh. Per token, that's 1/163934 = <a href='https://www.google.com/search?q=1%2F163934' target='_blank'>6.1e-6</a> kWh.",
    "emissions": {
      "co2e": 0.00000196522380956862
    }
  },
  {
    "id": "gpt-ruf-mortas-3b",
    "title": "Llama 3",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Llama 3 model with 8B parameters handles 201,527 completion tokens/kWh. Per token, that's 1/201527 = <a href='https://www.google.com/search?q=1%2F201527' target='_blank'>5.1e-6</a> kWh.",
    "emissions": {
      "co2e": 0.0000015986294615797499
    }
  },
  {
    "id": "gpt-ruf-mortas-token-2",
    "title": "GPT-3 emission per token (Ruf & Mortas)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\" target=\"_blank\">Llama 3</a>. The number of parameters in Llama 3 is 8 billion, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b>21.9x</b> more energy.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.",
    "emissions": {
      "co2e": 0.000002315423725774392
    }
  },
  {
    "id": "gpt-ruf-mortas-token",
    "title": "GPT-3 emission per token (Ruf & Mortas)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" target=\"_blank\">Llama 2</a>. The number of parameters in Llama 2 is 68.98 billion, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b>2.5x</b> more energy.",
    "emissions": {
      "co2e": 0.000002456529761960775
    }
  },
  {
    "id": "gpt-selvan-0",
    "title": "Monthly carbon footprint of ChatGPT (Selvan)",
    "description": "<a href='https://di.ku.dk/english/staff/?pure=en/persons/532407' target='_blank'>Raghavendra Selvan</a> from University of Copenhagen <a href='https://www.sueddeutsche.de/wissen/chat-gpt-energieverbrauch-ki-1.5780744' target='_blank'>approximated</a> the carbon footprint of ChatGPT. The calculation and the underlying assumptions he made are further illustrated in this <a href='https://github.com/borisruf/carbon-footprint-modeling-tool' target='_blank'>carbon footprint scenario</a>.<br/><br/>In January 2023, ChatGPT had about 100M unique users who made <a href='https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app' target='_blank'>590M</a> queries to GPT-3. The estimated emissions associated with these requests have been modeled below. To explore further, adjust the quantity in the input field or click on the link.",
    "emissions": {
      "co2e": 24240504,
      "co2": 13378014
    }
  },
  {
    "id": "gpt-selvan-0b",
    "title": "Monthly carbon footprint of ChatGPT (Selvan)",
    "description": "<a href='https://di.ku.dk/english/staff/?pure=en/persons/532407/' target='_blank'>Raghavendra Selvan</a> from University of Copenhagen <a href='https://www.sueddeutsche.de/wissen/chat-gpt-energieverbrauch-ki-1.5780744' target='_blank'>approximated</a> the carbon footprint of ChatGPT. The calculation and the underlying assumptions he made are further illustrated in this <a href='https://github.com/borisruf/carbon-footprint-modeling-tool' target='_blank'>carbon footprint scenario</a>.<br/><br/>In January 2023, ChatGPT had about 100M unique users who made <a href='https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app' target='_blank'>590M</a> queries to GPT-3. The estimated emissions associated with these requests have been modeled below. To explore further, adjust the quantity in the input field or click on the link.",
    "emissions": {
      "co2e": 36837219.11399999
    }
  },
  {
    "id": "gpt-selvan-1",
    "title": "GPT-3",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from a freely available open source language model (<a href='https://huggingface.co/docs/transformers/model_doc/gptj' target='_blank'>GPT-J</a>). Scaling to GPT-3, the following factors are taken into account:<br/><br/>1. The number of parameters in GPT-J is <a href='https://huggingface.co/EleutherAI/gpt-j-6b' target='_blank'>6 billion</a>, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>. By a conservative estimate, the larger model could require <b>10x</b> additional GPUs.<br/><br/>2. To fit the test environment, the GPT-J model is initialized in half-precision. The overhead of running a full precision model compared to half precision is about <b>1.5x</b> more.<br/><br/>Therefore, it is assumed that submitting a query to GPT-3 requires <b>15x</b> more energy compared to submitting a query to GPT-J.",
    "emissions": {
      "co2e": 0.0410856,
      "co2": 0.0226746
    }
  },
  {
    "id": "gpt-selvan-1b",
    "title": "Carbon footprint of one average query to GPT-3 (Selvan)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from a freely available open source language model (<a href='https://huggingface.co/docs/transformers/model_doc/gptj' target='_blank'>GPT-J</a>). Scaling to GPT-3, the following factors are taken into account:<br/><br/>1. The number of parameters in GPT-J is <a href='https://huggingface.co/EleutherAI/gpt-j-6b' target='_blank'>6 billion</a>, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>. By a conservative estimate, the larger model could require <b>10x</b> additional GPUs.<br/><br/>2. To fit the test environment, the GPT-J model is initialized in half-precision. The overhead of running a full precision model compared to half precision is about <b>1.5x</b> more.<br/><br/>Therefore, it is assumed that submitting a query to GPT-3 requires <b>15x</b> more energy compared to submitting a query to GPT-J.",
    "emissions": {
      "co2e": 0.06243596459999999
    }
  },
  {
    "id": "gpt-selvan-2",
    "title": "GPT-J",
    "description": "To estimate the carbon footprint of <a href=\"https://huggingface.co/docs/transformers/model_doc/gptj\" target=\"_blank\">GPT-J</a>, a freely available language model with 6 billion parameters, <a href=\"https://di.ku.dk/english/staff/?pure=en/persons/532407\" target=\"_blank\">Raghavendra Selvan</a> ran experiments on a local workstation. The model was initialized in half-precision (16 bit float instead of 32 bit float) to fit into GPU memory. The average length of text generated by the model was 230 words. The energy consumption got measured with <a href=\"https://github.com/lfwa/carbontracker/\" target=\"_blank\">CarbonTracker</a>, a tool developed by researchers from University of Copenhagen.<br><br>As a result, one request to GPT-J consumed in average <b>0.01292 kWh</b>.",
    "emissions": {
      "co2e": 0.00273904,
      "co2": 0.00151164
    }
  },
  {
    "id": "gpt-selvan-2b",
    "title": "GPT-J",
    "description": "To estimate the carbon footprint of <a href=\"https://huggingface.co/docs/transformers/model_doc/gptj\" target=\"_blank\">GPT-J</a>, a freely available language model with 6 billion parameters, <a href=\"https://di.ku.dk/english/staff/?pure=en/persons/532407\" target=\"_blank\">Raghavendra Selvan</a> ran experiments on a local workstation. The model was initialized in half-precision (16 bit float instead of 32 bit float) to fit into GPU memory. The average length of text generated by the model was 230 words. The energy consumption got measured with <a href=\"https://github.com/lfwa/carbontracker/\" target=\"_blank\">CarbonTracker</a>, a tool developed by researchers from University of Copenhagen.<br><br>As a result, one request to GPT-J consumed in average <b>0.01292 kWh</b>.",
    "emissions": {
      "co2e": 0.004162397639999999
    }
  },
  {
    "id": "gpt-selvan-token",
    "title": "GPT-3 emission per token (Selvan)",
    "description": "The outputs in this experiment generate in average 230 words. We apply factor <a href='https://www.google.com/search?q=1%2F230' target='_blank'>1/230</a> to get the emissions per completion token.<br/><br/>As this calculation ignores the impact of the prompt on the energy consumption, we assume an equal number of prompt tokens and apply factor <b>0.5x</b> to obtain a per-token value.",
    "emissions": {
      "co2e": 0.00013573035760891837
    }
  },
  {
    "id": "gpt-vries-0",
    "title": "Monthly carbon footprint of ChatGPT (de Vries)",
    "description": "One of the recent articles by Patel and Ahmed [31] assumes the number of active users for chatGPT to be 13 million and it was also assumed that 15 queries were made by each of the active users per day. = 195000000 per day",
    "emissions": {
      "co2e": 5465563.154999999
    }
  },
  {
    "id": "gpt-vries-1",
    "title": "Carbon footprint of one average query to GPT-3 (de Vries)",
    "description": "In the article '<a href= 'https://asociace.ai/wp-content/uploads/2023/10/ai-spotreba.pdf' target='_blank'>The growing energy footprint of artificial intelligence</a>' Alex de Vries from VU Amsterdam School of Business and Economics refers to estimations published in a <a href='https://www.semianalysis.com/p/the-inference-cost-of-search-disruption' target='_blank'>blog post</a> by  SemiAnalysis. They suggest that OpenAI needed 3,617 of NVIDIAâ€™s HGX A100 servers, totaling 28,936 graphics processing units (GPUs). From this, de Vries estimates a total energy demand of 564 MWh per day.<br><br> The same source also estimates that the number of daily active users is 13 million, with an average of 15 requests each, leading de Vries to estimate the average electricity consumption per request at 2.9 Wh. This aligns with a statement by Alphabet's chairman John Hennessy in February 2023, indicating that interacting with an LLM could '<a href='https://www.reuters.com/technology/tech-giants-ai-like-bing-bard-poses-billion-dollar-search-problem-2023-02-22/' target='_blank'>likely cost 10 times more than a standard keyword search,</a>' which was reported to be 0.3 Wh per query by Google in 2009.<br><br>Several reports, including one by <a href='https://www.goldmansachs.com/images/migrated/insights/pages/gs-research/gen-ai--too-much-spend,-too-little-benefit-/TOM_AI%202.0_ForRedaction.pdf' target='_blank'>Goldman Sachs Research</a> and another by <a href='https://www.bestbrokers.com/forex-brokers/ais-power-demand-calculating-chatgpts-electricity-consumption-for-handling-over-78-billion-user-queries-every-year/' target='_blank'>BestBrokers</a>, base their studies on this calculation.",
    "emissions": {
      "co2e": 0.0009342842999999998
    }
  },
  {
    "id": "gpt3-ruf-mortas-llama2-70b-token",
    "title": "GPT-3.5 per token â€“ Llama2 70B (Ruf & Mortas)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3.5, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" target=\"_blank\">Llama 2 70B</a>. The number of parameters in Llama 2 70B is 68.98 billion, whereas GPT-3.5 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b><a href=\"https://www.google.com/search?q=175/70\" target=\"blank\">2.5x</a></b> more energy.",
    "emissions": {
      "co2e": 0.000002456529761960775
    }
  },
  {
    "id": "gpt3-ruf-mortas-llama2-7b-token",
    "title": "GPT-3.5 per token â€“ Llama2 7B (Ruf & Mortas)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3.5, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" target=\"_blank\">Llama 2 7B</a>. The number of parameters in Llama 2 7B is 7 billion, whereas GPT-3.5 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b><a href=\"https://www.google.com/search?q=175/7\" target=\"blank\">25x</a></b> more energy.",
    "emissions": {
      "co2e": 0.0000045639546107583
    }
  },
  {
    "id": "gpt3-ruf-mortas-llama3-8b-token",
    "title": "GPT-3.5 per token â€“ Llama3 8B (Ruf & Mortas)",
    "description": "In the absence of reliable data on the energy consumption of GPT-3.5, this information is extrapolated from the freely available language model <a href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\" target=\"_blank\">Llama 3 8B</a>. The number of parameters in Llama 3 8B is 8 billion, whereas GPT-3.5 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>.<br/><br/>Therefore, we assume that submitting a query to GPT-3 requires <b><a href=\"https://www.google.com/search?q=175/8\" target=\"blank\">22x</a></b> more energy.",
    "emissions": {
      "co2e": 0.0000046519928737019756
    }
  },
  {
    "id": "gpt4-ruf-mortas-llama2-70b-token",
    "title": "GPT-4 per token â€“ Llama2 70B (Ruf & Mortas)",
    "description": "There is limited public information available about the technical structure of GPT4. <a href=\"https://x.com/swyx/status/1671272883379908608\" target=\"blank\">Some sources</a> suggest it is a <a href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" target=\"_blank\">Mixture of Expert</a> (MoE) with 8 x 220 billion parameters.<br/><br/>In other MoE architectures with disclosed weights, it has been noted that the actual total number of parameters is lower than indicated by the architecture. Additionally, it has been observed that only a fraction of those parameters are actually active during inference. For example, the <a href=\"https://docs.mistral.ai/getting-started/models/weights/#sizes/\" target=\"_blank\">Mixtral-8x22B-v0.3</a> architecture reports a total of 140.6B parameters (<a href=\"https://www.google.com/search?q=140.5/(8*22)\">80%</a>), out of which 39.1B parameters are activated during inference (<a href=\"https://www.google.com/search?q=39.1/140.6\" target=\"_blank\">28%</a>).<br/><br/>According to this information, we assume the same proportions for GPT-4 and thus estimate a total of <a href=\"https://www.google.com/search?q=8*220*0.8*0.28\" target=\"_blank\">394.24</a> billion active parameters.<br/><br/>Finally, we extrapolate the energy consumption of GPT-4 based on a scenario for GPT-3.5 which is known to have <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a> parameters, and therefore apply a factor of <a href=\"https://www.google.clom/search?q=394.24/175\" target=\"_blank\"><b>2.25x</b></a>.",
    "emissions": {
      "co2e": 0.000005527191964411744
    }
  },
  {
    "id": "gpt4-ruf-mortas-llama2-7b-token",
    "title": "GPT-4 per token â€“ Llama2 7B (Ruf & Mortas)",
    "description": "There is limited public information available about the technical structure of GPT4. <a href=\"https://x.com/swyx/status/1671272883379908608\" target=\"blank\">Some sources</a> suggest it is a <a href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" target=\"_blank\">Mixture of Expert</a> (MoE) with 8 x 220 billion parameters.<br/><br/>In other MoE architectures with disclosed weights, it has been noted that the actual total number of parameters is lower than indicated by the architecture. Additionally, it has been observed that only a fraction of those parameters are actually active during inference. For example, the <a href=\"https://docs.mistral.ai/getting-started/models/weights/#sizes/\" target=\"_blank\">Mixtral-8x22B-v0.3</a> architecture reports a total of 140.6B parameters (<a href=\"https://www.google.com/search?q=140.5/(8*22)\">80%</a>), out of which 39.1B parameters are activated during inference (<a href=\"https://www.google.com/search?q=39.1/140.6\" target=\"_blank\">28%</a>).<br/><br/>According to this information, we assume the same proportions for GPT-4 and thus estimate a total of <a href=\"https://www.google.com/search?q=8*220*0.8*0.28\" target=\"_blank\">394.24</a> billion active parameters.<br/><br/>Finally, we extrapolate the energy consumption of GPT-4 based on a scenario for GPT-3.5 which is known to have <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a> parameters, and therefore apply a factor of <a href=\"https://www.google.clom/search?q=394.24/175\" target=\"_blank\"><b>2.25x</b></a>.",
    "emissions": {
      "co2e": 0.000010268897874206174
    }
  },
  {
    "id": "gpt4-ruf-mortas-llama3-8b-token",
    "title": "GPT-4 per token â€“ Llama3 8B (Ruf & Mortas)",
    "description": "There is limited public information available about the technical structure of GPT4. <a href=\"https://x.com/swyx/status/1671272883379908608\" target=\"blank\">Some sources</a> suggest it is a <a href=\"https://en.wikipedia.org/wiki/Mixture_of_experts\" target=\"_blank\">Mixture of Expert</a> (MoE) with 8 x 220 billion parameters.<br/><br/>In other MoE architectures with disclosed weights, it has been noted that the actual total number of parameters is lower than indicated by the architecture. Additionally, it has been observed that only a fraction of those parameters are actually active during inference. For example, the <a href=\"https://docs.mistral.ai/getting-started/models/weights/#sizes/\" target=\"_blank\">Mixtral-8x22B-v0.3</a> architecture reports a total of 140.6B parameters (<a href=\"https://www.google.com/search?q=140.5/(8*22)\">80%</a>), out of which 39.1B parameters are activated during inference (<a href=\"https://www.google.com/search?q=39.1/140.6\" target=\"_blank\">28%</a>).<br/><br/>According to this information, we assume the same proportions for GPT-4 and thus estimate a total of <a href=\"https://www.google.com/search?q=8*220*0.8*0.28\" target=\"_blank\">394.24</a> billion active parameters.<br/><br/>Finally, we extrapolate the energy consumption of GPT-4 based on a scenario for GPT-3.5 which is known to have <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a> parameters, and therefore apply a factor of <a href=\"https://www.google.clom/search?q=394.24/175\" target=\"_blank\"><b>2.25x</b></a>.",
    "emissions": {
      "co2e": 0.000010466983965829445
    }
  },
  {
    "id": "gpt4o-mini-ruf-mortas-token",
    "title": "GPT-4o-mini emission per token (Ruf & Mortas)",
    "description": "Official information about the architecture of GPT-4o-mini is not available. In this scenario, we propose using our most optimistic energy consumption estimate made for GPT-4 and utilizing the price difference of both models as a proxy.<br/><br/>OpenAI charges <a href=\"https://openai.com/api/pricing/\" target=\"_blank\">different prices</a> for input and output tokens. Due to the mechanism of key-value (KV) caching, output tokens need <a href=\"https://www.greaterwrong.com/posts/PLf7tvzujaJ2A2r7N/what-the-cost-difference-in-processing-input-vs-output\" target=\"blank\">more memory</a> and therefore consume more energy than input tokens. For this reason, use the price for output tokens as proxy.</br></br>Currently, 1 million output tokens of GPT-4-turbo cost $30, while they cost $0.6 for GPT-4o-mini. Therefore, we assume a query to GPT-4o-mini to require <b><a href=\"https://www.google.com/search?q=0.6/30\" target=\"blank\">50x</a></b> less energy than for GPT-4.",
    "emissions": {
      "co2e": 2.0537795748412348e-7
    }
  },
  {
    "id": "gpt4o-ruf-mortas-token",
    "title": "GPT-4o emission per token (Ruf & Mortas)",
    "description": "Official information about the architecture of GPT-4o is not available. In this scenario, we propose using our most optimistic energy consumption estimate made for GPT-4 and utilizing the price difference of both models as a proxy.<br/><br/>OpenAI charges <a href=\"https://openai.com/api/pricing/\" target=\"_blank\">different prices</a> for input and output tokens. Due to the mechanism of key-value (KV) caching, output tokens need <a href=\"https://www.greaterwrong.com/posts/PLf7tvzujaJ2A2r7N/what-the-cost-difference-in-processing-input-vs-output\" target=\"blank\">more memory</a> and therefore consume more energy than input tokens. For this reason, use the price for output tokens as proxy.</br></br>Currently, 1 million output tokens of GPT-4-turbo cost $30, while they cost $10 for GPT-4o. Therefore, we assume a query to GPT-4o to require <b><a href=\"https://www.google.com/search?q=10/30\" target=\"blank\">3x</a></b> less energy than for GPT-4.",
    "emissions": {
      "co2e": 0.0000033887362984880375
    }
  },
  {
    "id": "llama2-70b-token",
    "title": "Llama 2 70B emission per token (Hugging Face)",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Llama 2 model with 68.98B parameters handles 163,934 completion tokens/kWh.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.<br/><br/>Per token, that's (1/163934)x0.5 = <a href='https://www.google.com/search?q=%281%2F163934%29x0.5' target='_blank'>3.05e-6</a> kWh.",
    "emissions": {
      "co2e": 9.8261190478431e-7
    }
  },
  {
    "id": "llama2-7b-token",
    "title": "Llama 2 7B emission per token (Hugging Face)",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Llama 2 model with 7B parameters handles 882,368 completion tokens/kWh.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.<br/><br/>Per token, that's (1/882368)x0.5 = <a href='https://www.google.com/search?q=%281%2F882368%29x0.5' target='_blank'>5.7e-7</a> kWh.",
    "emissions": {
      "co2e": 1.8255818443033198e-7
    }
  },
  {
    "id": "llama3-8b-token",
    "title": "Llama 3 8B emission per token (Hugging Face)",
    "description": "The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>Hugging Face LLM-Perf Leaderboard</a> benchmarks the performance of large language models (LLMs). Energy consumption is measured using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. We observe that the most energy efficient Llama 3 model with 8B parameters handles 761,789 completion tokens/kWh.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.<br/><br/>Per token, that's (1/761789)x0.5 = <a href='https://www.google.com/search?q=%281%2F761789%29x0.5' target='_blank'>6.6e-7</a> kWh.",
    "emissions": {
      "co2e": 2.11454221531908e-7
    }
  },
  {
    "id": "mistral-7b-token",
    "title": "Mistral 7B emission per token (Hugging Face)",
    "description": "The Hugging Face <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>LLM-Perf Leaderboard</a> does provide energy information for <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral 7B v0.1</a> which generates 720,589 completion tokens/kWh in its most energy efficient version and has <a href='https://docs.mistral.ai/getting-started/models/weights/#sizes'>7.3B parameters</a>.<br/><br/>Further note that in <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>the Hugging Face experiment</a>, the prompt size is 256 tokens, and the output size is 256 tokens, too. However, the \"Energy\" value refers to 1 generated completion token. As there is no reason to believe that prompt tokens consume no energy, we apply factor <b>0.5x</b> here, but use both prompt and completion tokens for our calculation.<br/><br/>Per token, that's (1/720589)x0.5 = <a href='https://www.google.com/search?q=%281%2F720589%29x0.5' target='_blank'>6.9e-7</a> kWh.",
    "emissions": {
      "co2e": 2.23544211858105e-7
    }
  },
  {
    "id": "mistral-large-2-ruf-mortas-token",
    "title": "Mistral Large 2 emission per token (Ruf & Mortas)",
    "description": "In the absence of energy consumption data of Mistral Large 2, this information is extrapolated from available data on <a href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" target=\"_blank\">Mistral 7B v0.1</a><br/><br/>The number of parameters in Mistral 7B is 7.3 billion, whereas Mistral Large 2 has a much larger parameter count of <a href=\"https://docs.mistral.ai/getting-started/models/weights/#sizes\" target=\"blank\"> 123 billion</a>.<br/><br/>Therefore, we assume that submitting a query to Mistral Large 2 requires <b><a href=\"https://www.google.com/search?q=123/7.3\" target=\"blank\">17x</a></b> more energy.",
    "emissions": {
      "co2e": 0.000003800251601587785
    }
  },
  {
    "id": "mistral-small-ruf-mortas-token",
    "title": "Mistral Small emission per token (Ruf & Mortas)",
    "description": "The Mistral Small model is not listed on the Hugging Face <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>LLM-Perf Leaderboard</a> which benchmarks the performance of large language models (LLMs) and measures the energy consumption using <a href='https://mlco2.github.io/codecarbon/index.html' target='_blank'>CodeCarbon</a>. However, as Mistral <a href='https://docs.mistral.ai/getting-started/models/weights/' target='_blank'>open source</a> the weights of their models, we know that Mistral Small has <a href='https://docs.mistral.ai/getting-started/models/weights/#sizes' target='_blank'>22B parameters</a>.<br/><br/>The <a href='https://huggingface.co/spaces/optimum/llm-perf-leaderboard' target='_blank'>leaderboard</a> does provide energy information for <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral 7B v0.1</a> which has <a href='https://docs.mistral.ai/getting-started/models/weights/#sizes' target='_blank'>7.3B parameters</a>. Accordingly, we propose to extrapolate this data and approximate the energy consumption of Mistral Small by applying a factor of <b><a href='https://www.google.com/search?q=22/7.3' target='_blank'>3x</a></b>.",
    "emissions": {
      "co2e": 6.70632635574315e-7
    }
  },
  {
    "id": "paris-dublin-plane",
    "title": "Paris to Dublin by plane",
    "emissions": {
      "co2e": 55.06157174722419
    }
  },
  {
    "id": "paris-dublin-train-ferry",
    "title": "Paris to Dublin by train and ferry",
    "description": "Paris â€“ London â€“ Holyhead â€“ Dublin",
    "emissions": {
      "co2e": 5.776515
    }
  },
  {
    "id": "scenario-0065da59-7785-4eed-8a11-c73b70cf798e",
    "title": "Singapore (SIN) to Paris (CDG) round-trip ",
    "emissions": {
      "co2": 1256.4401223726752,
      "ch4": 0.1757258912409336,
      "n2o": 0.010543553474456017
    }
  },
  {
    "id": "scenario-07ba767f-40e3-489c-8782-86f118957231",
    "title": "GPT-3",
    "description": "In the absence of reliable data on the energy consumption of GPT-3, this information is extrapolated from a freely available open source language model (<a href='https://huggingface.co/docs/transformers/model_doc/gptj' target='_blank'>GPT-J</a>). Scaling to GPT-3, the following factors are taken into account:<br/><br/>1. The number of parameters in GPT-J is <a href='https://huggingface.co/EleutherAI/gpt-j-6b' target='_blank'>6 billion</a>, whereas GPT-3 has a much larger parameter count of <a href='https://openai.com/research/language-models-are-few-shot-learners' target='_blank'>175 billion</a>. By a conservative estimate, the larger model could require <b>10x</b> additional GPUs.<br/><br/>2. To fit the test environment, the GPT-J model is initialized in half-precision. The overhead of running a full precision model compared to half precision is about <b>1.5x</b> more.<br/><br/>Therefore, it is assumed that submitting a query to GPT-3 requires <b>15x</b> more energy compared to submitting a query to GPT-J.",
    "emissions": {
      "co2e": 0.07189979999999999,
      "co2": 0.0683145
    }
  },
  {
    "id": "scenario-1L-hot-water",
    "title": "Warming 1L of water from 18 to 38Â°C",
    "description": "Before being heated, the temperature of the water in a tank is 18Â°C. <br> Most people take a shower at 38Â°C (a little bit above body temperature).<br><br> According to thermodynamic, heating 1L of water from 18 to 38Â°C consume 0.0232 kWh. <br><br><b>Formula:</b> Energy [kWh] = (mass of water [g] x specific heat capacity [J/gÂ°C] x delta T [Â°C]) / conversion coefficient [J/kWh] <br><b>Details:</b> (1 000 x 4.18 x (38 - 18)) / 3 600 000 = 0.0232",
    "emissions": {
      "co2e": 0.0012064
    }
  },
  {
    "id": "scenario-1b7dd417-5ac9-4be4-af6a-f2bb3bdae1a5",
    "title": "Food",
    "emissions": {
      "co2": 0
    }
  },
  {
    "id": "scenario-1hot-shower",
    "title": "1 hot shower",
    "description": "The average flow rate of a shower is 15L/minute. The average time for a shower in France is 9 minutes <a href='https://www.francetvinfo.fr/economie/les-francais-restent-9-minutes-en-moyenne-sous-la-douche_922471.html' target='_blank'>[source]</a>. <br>The volume of water used in a shower is 135 L. <br><br> <b>Formula:</b> Volume [L] = flow rate [L/min] x time [minutes] <br><b> Details:</b> 15 * 9 = 135",
    "emissions": {
      "co2e": 0.162864
    }
  },
  {
    "id": "scenario-265789b8-d7d1-442f-ba79-e627b9226c86",
    "title": "Personal carbon footprint",
    "emissions": {
      "co2": 3644.0672774038258
    }
  },
  {
    "id": "scenario-2a4d55c3-14ce-4c08-b24e-4de921def2e3",
    "title": "GPT-J",
    "description": "To estimate the carbon footprint of <a href=\"https://huggingface.co/docs/transformers/model_doc/gptj\" target=\"_blank\">GPT-J</a>, a freely available language model with 6 billion parameters, <a href=\"https://di.ku.dk/english/staff/?pure=en/persons/532407\" target=\"_blank\">Raghavendra Selvan</a> ran experiments on a local workstation. The model was initialized in half-precision (16 bit float instead of 32 bit float) to fit into GPU memory. The average length of text generated by the model was 230 words. The energy consumption got measured with <a href=\"https://github.com/lfwa/carbontracker/\" target=\"_blank\">CarbonTracker</a>, a tool developed by researchers from University of Copenhagen.<br><br>As a result, one request to GPT-J consumed in average <b>0.01292 kWh</b>.",
    "emissions": {
      "co2e": 0.004793319999999999,
      "co2": 0.0045543
    }
  },
  {
    "id": "scenario-32df3645-4e4e-40db-9066-aaf2c896a25e",
    "title": "Val's personal carbon footprint",
    "emissions": {
      "co2": 1884.6601835590127,
      "ch4": 0.2635888368614004,
      "n2o": 0.015815330211684027
    }
  },
  {
    "id": "scenario-55240d1b-3912-4fc6-a728-a4a8f56605c3",
    "title": "Cultivation of 1 kg of barley (15% water)",
    "emissions": {
      "co2e": 0.13435951123178838
    }
  },
  {
    "id": "scenario-615e4199-28fe-43d4-8b30-3cee5fe18923",
    "title": "Car pool 2022",
    "emissions": {
      "co2": 2657.7432551658403
    }
  },
  {
    "id": "scenario-6ae3dead-b8db-4383-af7a-dd16e8a199b2",
    "title": "Monthly carbon footprint of Santa Clara, CA",
    "description": "",
    "emissions": {
      "co2e": 41795296.606,
      "co2": 39711164.565
    }
  },
  {
    "id": "scenario-725b3ff2-294b-4cfc-81a3-fc460ee61fdc",
    "title": "Streaming a 30-minute video (IEA updated, Laptop and HD)",
    "emissions": {
      "co2": 0.010183973829123
    }
  },
  {
    "id": "scenario-8f35af7c-ee5b-42aa-b538-371b126b3d24",
    "title": "Streaming a 30-minute video (IEA updated, UK)",
    "emissions": {
      "co2e": 0.009340140964682,
      "co2": 0.007989294130946999
    }
  },
  {
    "id": "scenario-940200aa-f386-43e3-8c8d-f9cb8a8f900d",
    "title": "Monthly carbon footprint of Aarhus, Denmark",
    "emissions": {
      "co2e": 12448890,
      "co2": 13362570
    }
  },
  {
    "id": "scenario-95e1ade0-033c-40de-b30d-4e62f4723254",
    "title": "Mobility",
    "emissions": {
      "co2e": 2388.90234375,
      "co2": 1967.3470805625,
      "ch4": 0.26549893125,
      "n2o": 0.015929935875
    }
  },
  {
    "id": "scenario-98346938-bc01-4240-a256-20306e818745",
    "title": "Mobility",
    "emissions": {
      "co2": 2447.715277403826
    }
  },
  {
    "id": "scenario-999911f8-d853-4a69-a820-e3059bfd768e",
    "title": "Baking 1 kg of rye bread",
    "emissions": {
      "co2e": 0.17854650993358623
    }
  },
  {
    "id": "scenario-9e5f3acf-de9a-4952-a26f-db230c1fd88e",
    "title": "Cultivation of 1 ha of rye",
    "emissions": {
      "co2e": 3864.2513402000004
    }
  },
  {
    "id": "scenario-a03bc862-44f4-4ac6-be05-ea8ec93c1ba5",
    "title": "Car pool 2021",
    "emissions": {
      "co2e": 10061.391629464286,
      "co2": 8761.4680634625
    }
  },
  {
    "id": "scenario-a5f20019-1f40-4f41-b2a7-f9db5346a23f",
    "title": "Monthly carbon footprint of ChatGPT",
    "description": "<a href='https://di.ku.dk/english/staff/?pure=en/persons/532407' target='_blank'>Raghavendra Selvan</a> from University of Copenhagen <a href='https://www.sueddeutsche.de/wissen/chat-gpt-energieverbrauch-ki-1.5780744' target='_blank'>approximated</a> the carbon footprint of ChatGPT. The calculation and the underlying assumptions he made are further illustrated in this <a href='https://github.com/borisruf/carbon-footprint-modeling-tool' target='_blank'>carbon footprint scenario</a>.<br/><br/>In January 2023, ChatGPT had about 100M unique users who made <a href='https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app' target='_blank'>590M</a> queries to GPT-3. The estimated emissions associated with these requests have been modeled below. To explore further, adjust the quantity in the input field or click on the link.",
    "emissions": {
      "co2e": 42420881.99999999,
      "co2": 40305555
    }
  },
  {
    "id": "scenario-b9e32d49-98f1-40ed-b6ef-e96b90a4a186",
    "title": "Milling 1 kg of rye flour",
    "emissions": {
      "co2e": 0.0250425
    }
  },
  {
    "id": "scenario-bde03cd0-3f02-4431-9dca-9741f1753e67",
    "title": "Housing",
    "emissions": {
      "co2": 1196.3519999999999
    }
  },
  {
    "id": "scenario-business-travel-0",
    "title": "Monthly business travel in 2019",
    "description": "The AXA Group <a href='https://www-axa-com.cdn.axa-contento-118412.eu/www-axa-com/109c504e-bc3f-4e3a-bca0-5c3e1ccb65bb_AXA2022_Climate_and_Biodiversity-Report_Final_22_07_19.pdf' target='_blank'>published</a> a carbon footprint of 90.6kt CO2e for their business travel in 2019. To obtain the average monthly emissions, we use the fraction of <a href='https://www.google.com/search?q=1%2F12' target='_blank'>0.08333333333</a>.",
    "emissions": {
      "co2e": 7548666.666364719
    }
  },
  {
    "id": "scenario-c520e4d7-a190-44c4-abaa-8fa7b1c513c8",
    "title": "Paris (CDG) to Singapore (SIN) one-way",
    "emissions": {
      "co2": 628.2200611863376,
      "ch4": 0.0878629456204668,
      "n2o": 0.005271776737228008
    }
  },
  {
    "id": "scenario-cd8f70ff-8035-4801-9980-99e79d3e4a4b",
    "title": "Cultivation of 1 kg of rye (20% water)",
    "emissions": {
      "co2e": 0.1659827280088196
    }
  },
  {
    "id": "scenario-d9de099f-a408-4526-aec2-f781c9972b42",
    "title": "Cargo ship emissions portfolio 2021",
    "emissions": {
      "co2": 34588770
    }
  },
  {
    "id": "scenario-db3d851d-0c69-4663-af4a-bd1aad307a03",
    "title": "Monthly carbon footprint of population of Paris",
    "description": "Total <a href='https://www.worlddata.info/europe/france/energy-consumption.php' target='_blank'>energy consumption of France</a> per capita. <a href='https://www.insee.fr/fr/statistiques/1893198' target='_blank'>Estimated population</a> of Paris on 1 January 2023.",
    "emissions": {
      "co2e": 60595735.65745,
      "co2": 67947860.14035
    }
  },
  {
    "id": "scenario-e6c0c4d7-b9fb-4d76-bba0-5c6e4975dcf7",
    "title": "My car",
    "emissions": {
      "co2": 629.55106578,
      "ch4": 0.08495965800000001,
      "n2o": 0.0050975794799999995
    }
  },
  {
    "id": "scenario-ede88e6a-d8f3-4378-b5fe-9089965ed1d5",
    "title": "Yearly carbon footprint of Oldenburg, Germany",
    "description": "",
    "emissions": {
      "co2": 403444000
    }
  },
  {
    "id": "scenario-fecced87-a034-4fbd-adab-b0cfb6ab8053",
    "title": "Food",
    "emissions": {
      "co2": 0
    }
  },
  {
    "id": "scenario-habitation",
    "title": "Monthly carbon footprint of population of Lyon and its agglomeration",
    "description": "Total <a href='https://www.worlddata.info/europe/france/energy-consumption.php' target='_blank'>energy consumption of France</a> per capita. <a href='https://comersis.com/Les-metropoles-de-France-2023-actualite-4.html' target='_blank'>Estimated population</a> of Lyon Metropole on 2019.",
    "emissions": {
      "co2e": 40464065.3669,
      "co2": 45373599.716699995
    }
  },
  {
    "id": "scenario-hot-showers",
    "title": "A year of hot showers carbon footprint",
    "description": "Assuming that a human takes a shower each day, it takes 365 showers per year",
    "emissions": {
      "co2e": 59.44536
    }
  },
  {
    "id": "scenario-meat",
    "title": "Yearly meat consumption",
    "description": "In 2020, French people ate <a href='https://www.vitagora.com/en/blog/2021/flexitarian-meat-trend/#:~:text=(source%3A%20French%20Ministry%20of%20food,105kg%20per%20person%20in%201992.' target='_blank'>84.5kg</a> of meat per person. You can choose here how you want to distribute the kilogrammes between types of meat.",
    "emissions": {
      "co2e": 851.1999999999999
    }
  },
  {
    "id": "scenario-paris-montreal",
    "title": "Paris (CDG) to Montreal (YUL) round-trip ",
    "emissions": {
      "co2e": 654.1023315179427
    }
  },
  {
    "id": "scenario-short-distance-flights",
    "title": "KÃ¶ln-ZÃ¼rich Roundtrip",
    "description": "We consider that the flight distance roundtrip is  <a href='https://www.airmilescalculator.com/distance/cgn-to-zrh/' target='_blank'>784 km",
    "emissions": {
      "co2e": 349.57020761432,
      "co2": 75.187996884,
      "ch4": 0.010515803760000001,
      "n2o": 0.0006309482256
    }
  },
  {
    "id": "food-andouille",
    "title": "Andouille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouille-de-guemene",
    "title": "Andouille de GuÃ©mÃ©nÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouille-de-vire",
    "title": "Andouille de Vire (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouille-rechauffee-a-la-poele",
    "title": "Andouille/rÃ©chauffÃ©e Ã  la poÃªle (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouillette-crue",
    "title": "Andouillette/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouillette-de-troyes-crue",
    "title": "Andouillette de Troyes/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouillette-sautee-poelee",
    "title": "Andouillette/sautÃ©e/poÃªlÃ©e (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-boudin-blanc-truffe-cru",
    "title": "Boudin blanc truffÃ©/cru (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-boudin-noir-rayon-frais",
    "title": "Boudin noir/rayon frais (per kg)",
    "description": null,
    "emissions": {
      "co2e": 1.64
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-boudin-noir-saute-poele",
    "title": "Boudin noir/sautÃ©/poÃªlÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 1.64
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-bresaola",
    "title": "Bresaola (per kg)",
    "description": null,
    "emissions": {
      "co2e": 12.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-canard-magret-fume",
    "title": "Canard/magret fumÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-cervelas",
    "title": "Cervelas (per kg)",
    "description": null,
    "emissions": {
      "co2e": 11.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-cervelas-a-l'ail-pur-porc",
    "title": "Cervelas Ã  l'ail/pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.67
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-cervelas-obernois",
    "title": "Cervelas obernois (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.67
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chair-a-saucisse-crue",
    "title": "Chair Ã  saucisse/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.09
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chair-a-saucisse-porc-et-bÂœuf-crue",
    "title": "Chair Ã  saucisse/porc et bÂœuf/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.09
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chair-a-saucisse-pur-porc-crue",
    "title": "Chair Ã  saucisse/pur porc/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.09
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chipolata-crue",
    "title": "Chipolata/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.51
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chipolata-cuite",
    "title": "Chipolata/cuite (per kg)",
    "description": null,
    "emissions": {
      "co2e": 11.3
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chorizo",
    "title": "Chorizo (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.46
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chorizo-superieur-doux-ou-fort-type-charcuterie-en-tranches",
    "title": "Chorizo supÃ©rieur/doux ou fort/type charcuterie en tranches (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.17
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-chorizo-superieur-doux-ou-fort-type-saucisse-seche",
    "title": "Chorizo supÃ©rieur/doux ou fort, type saucisse sÃ¨che (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-confit-de-canard",
    "title": "Confit de canard (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.75
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-confit-de-canard-viande-cuisse-sans-peau-rechauffe",
    "title": "Confit de canard/viande (cuisse)/sans peau, rÃ©chauffÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.54
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-confit-de-foie-de-porc",
    "title": "Confit de foie de porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 1.92
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-confit-de-foie-de-volaille",
    "title": "Confit de foie de volaille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 1.58
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-coppa",
    "title": "Coppa (per kg)",
    "description": null,
    "emissions": {
      "co2e": 15.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-des-allumettes-rape-ou-hache-de-jambon",
    "title": "DÃ©s/allumettes, rÃ¢pÃ© ou hachÃ© de jambon (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.34
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-des-allumettes-rape-ou-hache-de-jambon-de-volaille",
    "title": "DÃ©s/allumettes, rÃ¢pÃ© ou hachÃ© de jambon de volaille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.47
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-diot-cru",
    "title": "Diot/cru (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.51
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-epaule-de-porc-cuite-choix-decouennee-degraissee",
    "title": "Ã‰paule de porc/cuite/choix, dÃ©couennÃ©e dÃ©graissÃ©e (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-epaule-de-porc-cuite-standard-decouennee-degraissee",
    "title": "Ã‰paule de porc/cuite/standard, dÃ©couennÃ©e dÃ©graissÃ©e (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-filet-de-bacon",
    "title": "Filet de bacon (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.36
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-fromage-de-tete",
    "title": "Fromage de tÃªte (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.44
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-hache-de-volaille",
    "title": "HachÃ© de volaille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-a-l'os-braise",
    "title": "Jambon Ã  l'os braisÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.03
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cru",
    "title": "Jambon cru (per kg)",
    "description": null,
    "emissions": {
      "co2e": 12.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cru-fume",
    "title": "Jambon cru/fumÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 14.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cru-fume-allege-en-matiere-grasse",
    "title": "Jambon cru/fumÃ©/allÃ©gÃ© en matiÃ¨re grasse (per kg)",
    "description": null,
    "emissions": {
      "co2e": 14.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-choix",
    "title": "Jambon cuit/choix (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-choix-avec-couenne",
    "title": "Jambon cuit/choix/avec couenne (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-choix-decouenne-degraisse",
    "title": "Jambon cuit/choix/dÃ©couennÃ© dÃ©graissÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-de-paris-decouenne-degraisse",
    "title": "Jambon cuit/de Paris/dÃ©couennÃ© dÃ©graissÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-fume",
    "title": "Jambon cuit/fumÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-superieur",
    "title": "Jambon cuit/supÃ©rieur (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.34
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-superieur-a-teneur-reduite-en-sel",
    "title": "Jambon cuit/supÃ©rieur/Ã  teneur rÃ©duite en sel (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-superieur-avec-couenne",
    "title": "Jambon cuit/supÃ©rieur/avec couenne (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-superieur-decouenne",
    "title": "Jambon cuit/supÃ©rieur/dÃ©couennÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit-superieur-decouenne-degraisse",
    "title": "Jambon cuit/supÃ©rieur/dÃ©couennÃ© dÃ©graissÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-de-bayonne",
    "title": "Jambon de Bayonne (per kg)",
    "description": null,
    "emissions": {
      "co2e": 12.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-de-dinde-ou-blanc-de-dinde-en-tranche",
    "title": "Jambon de dinde ou Blanc de dinde en tranche (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.47
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-de-porc-a-cuire-ou-jambon-a-rotir-cuire-au-four",
    "title": "Jambon de porc Ã  cuire ou Jambon Ã  rÃ´tir/cuire au four (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.34
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-de-poulet-ou-blanc-de-poulet-en-tranche",
    "title": "Jambon de poulet ou Blanc de poulet en tranche (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.47
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-en-croute",
    "title": "Jambon en croÃ»te (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.83
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambonneau-cuit",
    "title": "Jambonneau/cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.03
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-sec",
    "title": "Jambon sec (per kg)",
    "description": null,
    "emissions": {
      "co2e": 15.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-sec-decouenne-degraisse",
    "title": "Jambon sec/dÃ©couennÃ©/dÃ©graissÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 17.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-sec-de-parme",
    "title": "Jambon sec de Parme (per kg)",
    "description": null,
    "emissions": {
      "co2e": 17.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-sec-serrano",
    "title": "Jambon sec Serrano (per kg)",
    "description": null,
    "emissions": {
      "co2e": 17.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-lardon-fume-cru",
    "title": "Lardon fumÃ©/cru (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.66
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-lardon-fume-cuit",
    "title": "Lardon fumÃ©/cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.21
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-lardon-nature-cru",
    "title": "Lardon nature/cru (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.66
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-lardon-nature-cuit",
    "title": "Lardon nature/cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.21
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-boeuf-et-mouton-crue",
    "title": "Merguez/boeuf et mouton, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 47.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-boeuf-et-mouton-cuite",
    "title": "Merguez/boeuf et mouton, cuite (per kg)",
    "description": null,
    "emissions": {
      "co2e": 47.8
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-bÂœuf-mouton-et-porc-crue",
    "title": "Merguez/bÂœuf/mouton et porc, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 32.3
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-crue",
    "title": "Merguez/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 28.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-porc-et-bÂœuf-crue",
    "title": "Merguez/porc et bÂœuf/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 25
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-pur-bÂœuf-crue",
    "title": "Merguez/pur bÂœuf/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 30.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mortadelle",
    "title": "Mortadelle (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mortadelle-pistachee-pur-porc",
    "title": "Mortadelle pistachÃ©e pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.18
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mortadelle-porc-et-boeuf",
    "title": "Mortadelle/porc et boeuf (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.18
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mortadelle-pur-porc",
    "title": "Mortadelle/pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.18
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mousse-de-canard",
    "title": "Mousse de canard (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mousse-de-foie-de-porc",
    "title": "Mousse de foie de porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.91
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-mousse-de-foie-de-porc-superieure-ou-creme-de-foie",
    "title": "Mousse de foie de porc supÃ©rieure ou CrÃ¨me de foie (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.51
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-museau-de-boeuf-en-vinaigrette",
    "title": "Museau de boeuf en vinaigrette (per kg)",
    "description": null,
    "emissions": {
      "co2e": 27.1
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-museau-de-porc-vinaigrette",
    "title": "Museau de porc vinaigrette (per kg)",
    "description": null,
    "emissions": {
      "co2e": 1.98
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-oreille-de-porc-demi-sel",
    "title": "Oreille de porc demi-sel (per kg)",
    "description": null,
    "emissions": {
      "co2e": 2.33
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pancetta-ou-poitrine-roulee-seche",
    "title": "Pancetta ou Poitrine roulÃ©e sÃ¨che (per kg)",
    "description": null,
    "emissions": {
      "co2e": 15.9
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-au-poivre-vert",
    "title": "PÃ¢tÃ© au poivre vert (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-breton",
    "title": "PÃ¢tÃ© breton (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-foie-de-porc",
    "title": "PÃ¢tÃ© de foie de porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-foie-de-porc-superieur",
    "title": "PÃ¢tÃ© de foie de porc/supÃ©rieur (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-foie-de-volaille",
    "title": "PÃ¢tÃ© de foie de volaille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-foie-d'oie",
    "title": "PÃ¢tÃ© de foie d'oie (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-gibier",
    "title": "PÃ¢tÃ© de gibier (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-de-lapin",
    "title": "PÃ¢tÃ© de lapin (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.14
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-en-croute",
    "title": "PÃ¢tÃ© en croÃ»te (per kg)",
    "description": null,
    "emissions": {
      "co2e": 13.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-ou-terrine-aux-champignons-forestier",
    "title": "PÃ¢tÃ© ou terrine aux champignons (forestier) (per kg)",
    "description": null,
    "emissions": {
      "co2e": 2.48
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pate-ou-terrine-de-campagne",
    "title": "PÃ¢tÃ© ou terrine de campagne (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-pied-de-porc-demi-sel",
    "title": "Pied de porc demi-sel (per kg)",
    "description": null,
    "emissions": {
      "co2e": 3.02
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-poitrine-de-porc-demi-sel",
    "title": "Poitrine de porc demi-sel (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.21
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-poitrine-de-porc-fumee-crue",
    "title": "Poitrine de porc/fumÃ©e, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.56
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-de-poisson-crue",
    "title": "Quenelle de poisson/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.86
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-de-poisson-en-sauce",
    "title": "Quenelle de poisson/en sauce (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.92
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-de-veau-en-sauce",
    "title": "Quenelle de veau/en sauce (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.51
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-de-volaille-crue",
    "title": "Quenelle de volaille/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-de-volaille-en-sauce",
    "title": "Quenelle de volaille/en sauce (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.46
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-quenelle-nature-crue",
    "title": "Quenelle nature/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 3.57
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-de-canard",
    "title": "Rillettes de canard (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-de-poulet",
    "title": "Rillettes de poulet (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-de-tours",
    "title": "Rillettes de Tours (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-d'oie",
    "title": "Rillettes d'oie (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-du-mans",
    "title": "Rillettes du Mans (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-pur-oie",
    "title": "Rillettes pur oie (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-pur-porc",
    "title": "Rillettes pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes-traditionnelles-de-porc",
    "title": "Rillettes traditionnelles de porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rond-de-jambon-cuit",
    "title": "Rond de jambon cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.03
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rosette-ou-fuseau",
    "title": "Rosette ou Fuseau (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-roti-de-volaille-en-salaison-cuit",
    "title": "RÃ´ti de volaille en salaison/cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.31
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-roulade-de-porc-pistachee",
    "title": "Roulade de porc pistachÃ©e (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.51
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-salami",
    "title": "Salami (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-salami-porc-et-boeuf",
    "title": "Salami porc et boeuf (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.45
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-salami-pur-porc",
    "title": "Salami pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.08
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-salami-type-danois",
    "title": "Salami type danois (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.08
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-alsacienne-fumee-ou-gendarme",
    "title": "Saucisse alsacienne fumÃ©e ou Gendarme (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-cocktail",
    "title": "Saucisse cocktail (per kg)",
    "description": null,
    "emissions": {
      "co2e": 14.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-biere",
    "title": "Saucisse de biÃ¨re (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.67
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-foie",
    "title": "Saucisse de foie (per kg)",
    "description": null,
    "emissions": {
      "co2e": 12.5
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-francfort",
    "title": "Saucisse de Francfort (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.67
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-jambon-pur-porc",
    "title": "Saucisse de jambon pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 8.67
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-montbeliard",
    "title": "Saucisse de MontbÃ©liard (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-morteau",
    "title": "Saucisse de Morteau (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-morteau-bouillie-cuite-a-l'eau",
    "title": "Saucisse de Morteau/bouillie/cuite Ã  l'eau (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-strasbourg-ou-knack",
    "title": "Saucisse de Strasbourg ou Knack (per kg)",
    "description": null,
    "emissions": {
      "co2e": 14.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-toulouse-crue",
    "title": "Saucisse de Toulouse/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-toulouse-cuite",
    "title": "Saucisse de Toulouse/cuite (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16.2
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-volaille-facon-charcutiere",
    "title": "Saucisse de volaille/faÃ§on charcutiÃ¨re (per kg)",
    "description": null,
    "emissions": {
      "co2e": 13.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-de-volaille-type-knack",
    "title": "Saucisse de volaille/type Knack (per kg)",
    "description": null,
    "emissions": {
      "co2e": 17.6
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-seche",
    "title": "Saucisse sÃ¨che (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.45
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-viennoise-crue",
    "title": "Saucisse viennoise/crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-a-l'ail",
    "title": "Saucisson Ã  l'ail (per kg)",
    "description": null,
    "emissions": {
      "co2e": 11.8
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-brioche-cuit",
    "title": "Saucisson briochÃ©/cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.89
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-cuit-pur-porc",
    "title": "Saucisson cuit pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.88
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-de-paris",
    "title": "Saucisson de Paris (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.46
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-de-paris-fume",
    "title": "Saucisson de Paris/fumÃ© (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.46
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-sec",
    "title": "Saucisson sec (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-sec-aux-noix-et-ou-noisettes",
    "title": "Saucisson sec aux noix et/ou noisettes (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.45
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-sec-pur-porc",
    "title": "Saucisson sec pur porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisson-sec-pur-porc-qualite-superieure",
    "title": "Saucisson sec pur porc/qualitÃ© supÃ©rieure (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.4
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-terrine-de-canard",
    "title": "Terrine de canard (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.18
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-terrine-de-lapin",
    "title": "Terrine de lapin (per kg)",
    "description": null,
    "emissions": {
      "co2e": 5.21
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-andouillette-andouille",
    "title": "Andouillette/Andouille (per kg)",
    "description": null,
    "emissions": {
      "co2e": 16
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-boudin-noir",
    "title": "Boudin noir (per kg)",
    "description": null,
    "emissions": {
      "co2e": 9.62
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-epaule-de-porc",
    "title": "Ã‰paule de porc (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.34
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-jambon-cuit",
    "title": "Jambon cuit (per kg)",
    "description": null,
    "emissions": {
      "co2e": 6.34
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-boeuf-mouton-et-porc-crue",
    "title": "Merguez/boeuf, mouton et porc, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 38.3
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-porc-et-boeuf-crue",
    "title": "Merguez/porc et boeuf, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 28.7
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-merguez-pur-boeuf-crue",
    "title": "Merguez/pur boeuf, crue (per kg)",
    "description": null,
    "emissions": {
      "co2e": 36.8
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-rillettes",
    "title": "Rillettes (per kg)",
    "description": null,
    "emissions": {
      "co2e": 4.84
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  },
  {
    "id": "food-saucisse-suisse-a-cuire",
    "title": "Saucisse suisse Ã  cuire (per kg)",
    "description": null,
    "emissions": {
      "co2e": 7.26
    },
    "url": "https://base-empreinte.ademe.fr/donnees/jeu-donnees"
  }
]